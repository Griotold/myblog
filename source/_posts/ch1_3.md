---
title: "혼자서 공부하는 머신러닝 + 딥러닝_ch1_3 "
author: "HS"
date: '2022-03-28'
categories: 'Python'
---

# 혼자서 공부하는 머신러닝 + 딥러닝
## 44-64 페이지 읽고 정리
<!-- more-->
## 첫 번째 임무 : 생선 이름을 자동으로 알려주는 머신러닝 만들기
## 생선 분류 문제
- 생선 데이터셋 출처 : https://www.kaggle.com/aungpyaeap/fish-market
- 생선 길이가 30cm 이상이면 도미다.


```python
if fish_length >= 30:
  print("도미")
```

- 머신러닝은 스스로 기준을 찾아서 일을 한다.
- 머신러닝은 기준을 찾을 뿐만 아니라 이 기준을 이용해 생선이 도미인지 아닌지 판별할 수도 있다.
### 도미 데이터 준비하기
- 먼저, 도미와 빙어를 구분해보자. Bream VS Smelt
- 주의! 여러 개의 종류(클래스) 중 하나를 구별하는 문제를 분류라고 부른다.
- 2개의 클래스 중 하나를 고르는 문제를 이진 분류(binary classification)라고 한다. 여기서 클래스는 파이썬에서 클래스와는 다르다.


```python
# http://bit.ly/bream_list

# 도미의 길이 데이터
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]

# 도미의 무게 데이터
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]


```


    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    <ipython-input-6-edf837b3e2f5> in <module>()
         12 
         13 bream_df = [bream_length, bream_weight]
    ---> 14 bream_df.shape()
    

    AttributeError: 'list' object has no attribute 'shape'


- feature(특성)은 데이터의 특징이다. 첫 번째 도미의 특성은 길이가 25.4cm이고, 무게는 242.0g이다.
- 두 특성을 좀 더 명확히 이해하기 위해 산점도 그래프(scatter plot)로 표현해보자.


```python
import matplotlib.pyplot as plt # matplotlib의 pyplot 함수를 plt로 앨리어싱

plt.scatter(bream_length, bream_weight)
plt.xlabel('length') # x축은 길이
plt.ylabel('weight') # y축은 무게
plt.show()
```


    
![png](/images/ch1_3/output_6_0.png)
    


### 빙어 데이터 준비하기


```python
# http://bit.ly/smelt_list
# 빙어의 길이
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

# 빙어의 무게
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

- 산점도 그래프


```python
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length') # x축은 길이
plt.ylabel('weight') # y축은 무게
plt.show()
```


    
![png](/images/ch1_3/output_10_0.png)
    


## 첫 번째 머신러닝 프로그램
### K-최근접 이웃 알고리즘(K-Nearest Neighbors)

- 도미와 빙어 데이터 합치기
- 파이썬에서는 두 리스트를 더하면 하나의 리스트가 된다.


```python
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
print(length)
print(weight)
```

    [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
    [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
    

- 사이킷런 머신러닝 패키지를 사용하기 위해 세로 방향 2차원 리스트로 만들기
- zip() 함수와 리스트 내포 구문(list comprehension)


```python
fish_data = [[l, w] for l, w in zip(length, weight)]
print(fish_data)
```

    [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
    

### zip() 함수
- 출처 : https://www.daleseo.com/python-zip/
- 마치 점퍼의 zipper 처럼 두 그룹의 데이터를 서로 엮어주는 파이썬의 내장 함수


```python
# 예제
numbers = [1, 2, 3]
letters = ["A", "B", "C"]
for pair in zip(numbers, letters):
  print(pair) # zip()함수는 튜플 형태로 반환한다.
```

    (1, 'A')
    (2, 'B')
    (3, 'C')
    


```python
# zip() 함수를 사용하지 않고, index 변수를 사용하는 방법
numbers = [1, 2, 3]
letters = ["A", "B", "C"]
for i in range(3):
  pair = (numbers[i], letters[i])
  print(pair)
```

    (1, 'A')
    (2, 'B')
    (3, 'C')
    

### 2차원 리스트 혹은 리스트의 리스트
- fish_data처럼, 각각의 데이터가 리스트이며, 그 데이터들이 모여 있는 리스트를 2차원 리스트 혹은 리스트의 리스트라 한다.

### 정답 데이터 만들기
- 머신러닝은 구분하는 규칙 찾기를 원한다. 그렇게 하려면 정답을 알려줘야 한다.
- 도미는 1, 빙어는 0으로 만들어준다.
- 관습적으로 원하는 답을 1로, 그 외는 0으로 놓는다.


```python
# 정답 데이터
fish_target = [1] * 35 + [0] * 14
print(fish_target)
```

    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    

- KNeighborsClassifier 임포트
- 파이썬에서 패키지나 모듈 전체를 임포트하지 않고 특정 클래스만 임포트 하려면 from ~ import 구문을 사용한다.


```python
from sklearn.neighbors import KNeighborsClassifier

# 객체 만들기
kn = KNeighborsClassifier()
```

### training (훈련)
- 이 객체에 fish_data와 fish_target을 전달하여 도미를 찾기 위한 기준을 학습시킨다.
- 사이킷런에서는 fit()메서드가 훈련을 시킨다.


```python
kn.fit(fish_data, fish_target)
```




    KNeighborsClassifier()



### 평가
- 객체(또는 모델) kn이 얼마나 잘 훈련되었는지 평가해보자.
- 사이킷런에서 모델을 평가하는 메서드는 score()이다.


```python
kn.score(fish_data, fish_target)
```




    1.0



### 정확도(accuracy)
- 정확도가 1.0
- 이 모델의 정확도가 100%
- 도미와 빙어를 완벽하게 분류했다.

### K-최근접 이웃 알고리즘
- 어떤 데이터에 대한 답을 구할 떄 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용한다.
- predict() 메서드는 새로운 데이터의 정답을 예측한다. fit() 메서드와 마찬가지로 2차원 리스트를 전달해야 한다.


```python
# predict() 메서드
kn.predict([[30, 600]])
```




    array([1])



- 30cm에 600g인 생선은 도미로 예측했다.

### 사이킷런의 KNeighborsClassifier 클래스



```python
print(kn._fit_X)
```

    [[  25.4  242. ]
     [  26.3  290. ]
     [  26.5  340. ]
     [  29.   363. ]
     [  29.   430. ]
     [  29.7  450. ]
     [  29.7  500. ]
     [  30.   390. ]
     [  30.   450. ]
     [  30.7  500. ]
     [  31.   475. ]
     [  31.   500. ]
     [  31.5  500. ]
     [  32.   340. ]
     [  32.   600. ]
     [  32.   600. ]
     [  33.   700. ]
     [  33.   700. ]
     [  33.5  610. ]
     [  33.5  650. ]
     [  34.   575. ]
     [  34.   685. ]
     [  34.5  620. ]
     [  35.   680. ]
     [  35.   700. ]
     [  35.   725. ]
     [  35.   720. ]
     [  36.   714. ]
     [  36.   850. ]
     [  37.  1000. ]
     [  38.5  920. ]
     [  38.5  955. ]
     [  39.5  925. ]
     [  41.   975. ]
     [  41.   950. ]
     [   9.8    6.7]
     [  10.5    7.5]
     [  10.6    7. ]
     [  11.     9.7]
     [  11.2    9.8]
     [  11.3    8.7]
     [  11.8   10. ]
     [  11.8    9.9]
     [  12.     9.8]
     [  12.2   12.2]
     [  12.4   13.4]
     [  13.    12.2]
     [  14.3   19.7]
     [  15.    19.9]]
    


```python
print(kn._y)
```

    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
     0 0 0 0 0 0 0 0 0 0 0 0]
    

- 실제로 k-최근접 이웃 알고리즘은 무언가 훈련되는 게 없는 셈이다. fit() 메서드에 전달한 데이터를 모두 저장하고 있다가 새로운 데이터가 등장하면 가장 가까운 데이터를 참고하여 도미인지 빙어인지를 구분한다.


```python
# 참고 데이터를 49개로 한 kn49모델
# 기본값은 5
kn49 = KNeighborsClassifier(n_neighbors=49)
```

- 49개를 참고 데이터로 하면, 그 중에 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 예측하게 된다.



```python
# 어떤 데이터를 넣어도 무조건 도미라고 말하는 알고리즘

kn49.fit(fish_data, fish_target)
print(kn49.score(fish_data, fish_target))
print(35/49)
```

    0.7142857142857143
    0.7142857142857143
    
